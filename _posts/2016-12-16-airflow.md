---
layout: post
title: "airflow 简明指南"
description: ""
category: 
comments: true
tags: [tech, pip, python]
---
{% include JB/setup %}

最近被线上任务折磨得不行了，总是隔三差五出各种问题，导致日志丢了或者脚本没跑成功，
出了问题就需要手动去修复，比如手动把少的日志补齐，重跑失败的脚本。有些脚本之间有依赖关系，
手动跑起来就比较复杂，需要一会看一眼脚本有没有跑完，再接着跑下一个，严重影响效率。
在此之前，我在脑海中已经设想好我将要得到的是什么，我应该画一张有向无环图（DAG）
表示任务的依赖关系，甚至可以依赖前一时间段的自己，对于每个任务我可以定义如何执行，每小时，每天等等，
它会自动将标准输出和标准错误定向到命名好的文件便于查看。最关键的是，可以自动完成依赖关系的计算，
当重跑某个节点的任务后，它的后续被影响到的结点会自动重跑。如果能分布式地执行就更好了。
最近找到一个满足现在这些需求的开源项目—— airflow
<!--more-->
，刚开始用的时候也踩了不少坑，
但总体用起来还是不禁赞叹：“哎玛，这不就是我心中想的那样么！”

---

## 安装
airflow 的安装十分简单，用 `pip` 轻松搞定

```shell
export AIRFLOW_HOME=~/airflow
pip install airflow[slack]
airflow initdb
```

pip 安装的 slackclient 为可选，当你需要通知到 slack 时才会用到，但我十分建议也一起安装，
能够及时收到任务执行状况报告。

---

## Quick Start

不得不说，airflow 的文档非常完善，从快速入门到整个框架的概念解释都很到们。
看完官方的 [tutorial](https://airflow.incubator.apache.org/tutorial.html)就可以开始干活了。
如前所说，我需要先设置一个 DAG 对象的一些属性，比如重试策略，起止时间，执行环境等，
就像这样：

```python
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2015, 6, 1),
    'email': ['airflow@airflow.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
}
```

参数看描述基本都可以理解，`depends_on_past` 表示就是是否依赖上一个自己的执行状态。
如果设置了 `email` 相关的配置，需要在 `airflow.cfg` 中配置下发件邮箱。

以上只是配置了 DAG 的参数，下面就建立了一个 dag 对象：

```python
dag = DAG(
    'tutorial', default_args=default_args, schedule_interval='* * * * *')
```

这里我修改了下官方的例子，`schedule_interval` 表示执行的周期，
我改成了 crontab的形式，这样更直观也方便修改，
airflow 也提供一些字面意思的值表示执行周期，比如`@hourly`等，但如果真在线上执行，
显然我们一般会将不同脚本错锋执行，不会全设成X点0分执行，所以我建议用 crontab形式的写法更好。

下面就开始定义任务了，任务并不是像我想像的那样扔一个 shell 脚本或者一个 python 脚本就行了，
不同的脚本对应了不同的 Operator，比如 shell 就需要用 BashOperator 来执行。就像这样：

```python
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag)

text = '{{ ds }} [%s] has been done' % (dag.dag_id)
t2 = SlackAPIPostOperator(
    task_id='post_slack',
    token='xoxp-your-key-here',
    channel='#random',
    username='airflow',
    text=text,
    dag=dag
)

t1 >> t2  # t1.set_upstream(t2)
```

我又修改了下例子，这个 dag 包含两个任务 t1 和 t2，t1 是个 shell 命令，调用 `date`显示当前时间，
t2 是个发往 slack 的操作，需要设置一个 slack token，可以从[这里](https://api.slack.com/web)获得，
接着设置发往的 channel和用户名，保持原样就好，发 slack 消息就需要刚才安装的时候装了 slackclient。

然后再看一眼发的消息`text`，airflow 执行的命令或这种消息是支持 jinja2 模板语言，
`{{ ds }}`是一种宏，表示当前的日期，形如`2016-12-16`，支持的宏在[这里](https://airflow.incubator.apache.org/code.html#macros)。

最后一行就是设置依赖关系，显而易见，这是 t1 先执行，t2 在 t1 完成后执行，
也可以用注释里的写法，但我觉得`>>`这样更直观，反之还有`<<`。
如果有多条依赖，只需要分行写就行了，没什么特别的：

```python
t1 >> t
t3 >> t << t2
t >> w >> x
```

以上的依赖关系图就像这样：

```
t1 +----+
        |
t2 +-------> t +----> w +----> x
        |
t3 +----+
```

以上，恭喜你已经成功创建了第一个 DAG 图，下面就可以开始执行了！

## 命令

